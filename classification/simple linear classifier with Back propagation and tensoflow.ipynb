{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/motoofi/miniconda2/envs/mlp/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, 112], 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [None,1], 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.zeros([112, 1]))\n",
    "biases = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.matmul(inputs, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_datapoint_errors = tf.nn.softmax_cross_entropy_with_logits(logits = outputs, labels = targets)\n",
    "error = tf.reduce_mean(per_datapoint_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_datapoint_pred_is_correct = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(per_datapoint_pred_is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_train = pd.read_csv(\"./X_train.csv\")\n",
    "X_test = pd.read_csv(\"./X_test.csv\")\n",
    "data_y_train = pd.read_csv(\"./y_train.csv\")\n",
    "\n",
    "_CSV_COLUMNS = [str(col) for col in data_x_train.columns]\n",
    "_CSV_COLUMNS.append('label')\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data_x_train, data_y_train, test_size = 0.2, random_state = 0)\n",
    "\n",
    "XY_train = pd.concat([X_train,Y_train['EpiOrStroma']], axis=1) \n",
    "XY_val = pd.concat([X_val,Y_val['EpiOrStroma']], axis=1) \n",
    "X_test['label'] = 0. \n",
    "\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "XY_train_scaled = pd.DataFrame(sc_X.fit_transform(XY_train), columns = _CSV_COLUMNS)\n",
    "XY_train_scaled.loc[XY_train_scaled['label']>0,'label'] = 1\n",
    "XY_train_scaled.loc[XY_train_scaled['label']<0,'label'] = 0\n",
    "train_data = np.split(ary = XY_train_scaled.values ,indices_or_sections = 10 ,axis=0)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(sc_X.transform(X_test), columns = _CSV_COLUMNS)\n",
    "#del X_test_scaled['label']\n",
    "\n",
    "\n",
    "XY_val_scaled = pd.DataFrame(sc_X.transform(XY_val), columns = _CSV_COLUMNS)\n",
    "XY_val_scaled.loc[XY_val_scaled['label']>0,'label'] = 1\n",
    "XY_val_scaled.loc[XY_val_scaled['label']<0,'label'] = 0\n",
    "valid_data = np.split(XY_val_scaled.values,10,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1: running error average = 0.00\n",
      "End of epoch 2: running error average = 0.00\n",
      "End of epoch 3: running error average = 0.00\n",
      "End of epoch 4: running error average = 0.00\n",
      "End of epoch 5: running error average = 0.00\n",
      "End of epoch 6: running error average = 0.00\n",
      "End of epoch 7: running error average = 0.00\n",
      "End of epoch 8: running error average = 0.00\n",
      "End of epoch 9: running error average = 0.00\n",
      "End of epoch 10: running error average = 0.00\n",
      "End of epoch 11: running error average = 0.00\n",
      "End of epoch 12: running error average = 0.00\n",
      "End of epoch 13: running error average = 0.00\n",
      "End of epoch 14: running error average = 0.00\n",
      "End of epoch 15: running error average = 0.00\n",
      "End of epoch 16: running error average = 0.00\n",
      "End of epoch 17: running error average = 0.00\n",
      "End of epoch 18: running error average = 0.00\n",
      "End of epoch 19: running error average = 0.00\n",
      "End of epoch 20: running error average = 0.00\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 20\n",
    "for e in range(num_epoch):\n",
    "    running_error = 0.\n",
    "    for input_batch in train_data:\n",
    "        _, batch_error = sess.run(\n",
    "            [train_step, error], \n",
    "            feed_dict={inputs: input_batch[:,:-1], targets: input_batch[:,112].reshape((48,1))})\n",
    "        running_error += batch_error\n",
    "    running_error /= len(train_data[0])\n",
    "    print('End of epoch {0}: running error average = {1:.2f}'.format(e + 1, running_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
